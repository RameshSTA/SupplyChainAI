# SupplyChain AI

<p align="center">
  <a href="https://supplychainai.streamlit.app/" target="_blank">
    <img src="https://img.shields.io/badge/Live_Demo-Streamlit_Cloud-FF4B4B?style=for-the-badge&logo=streamlit" alt="Live Demo">
  </a>
</p>

<p align="center">
  <a href="https://www.python.org/downloads/release/python-390/" target="_blank"><img src="https://img.shields.io/badge/Python-3.9%2B-blue.svg?style=for-the-badge&logo=python&logoColor=white" alt="Python Version"></a>
  <a href="https://streamlit.io" target="_blank"><img src="https://img.shields.io/badge/Streamlit-%23FF4B4B.svg?style=for-the-badge&logo=Streamlit&logoColor=white" alt="Streamlit"></a>
  <a href="https://pandas.pydata.org" target="_blank"><img src="https://img.shields.io/badge/Pandas-%23150458.svg?style=for-the-badge&logo=pandas&logoColor=white" alt="Pandas"></a>
  <a href="https://numpy.org" target="_blank"><img src="https://img.shields.io/badge/NumPy-%23013243.svg?style=for-the-badge&logo=numpy&logoColor=white" alt="NumPy"></a>
  <a href="https://scikit-learn.org" target="_blank"><img src="https://img.shields.io/badge/Scikit--learn-%23F7931E.svg?style=for-the-badge&logo=scikit-learn&logoColor=white" alt="Scikit-learn"></a>
  <a href="https://www.tensorflow.org" target="_blank"><img src="https://img.shields.io/badge/TensorFlow-%23FF6F00.svg?style=for-the-badge&logo=TensorFlow&logoColor=white" alt="TensorFlow"></a>
  <a href="https://keras.io" target="_blank"><img src="https://img.shields.io/badge/Keras-%23D00000.svg?style=for-the-badge&logo=Keras&logoColor=white" alt="Keras"></a>
  <a href="https://www.nltk.org" target="_blank"><img src="https://img.shields.io/badge/NLTK-%230C59A3.svg?style=for-the-badge&logo=nltk&logoColor=white" alt="NLTK"></a>
  <a href="https://facebook.github.io/prophet/" target="_blank"><img src="https://img.shields.io/badge/Prophet-%23007F7F.svg?style=for-the-badge&logo=facebook&logoColor=white" alt="Prophet"></a>
  <a href="https://alkaline-ml.com/pmdarima/" target="_blank"><img src="https://img.shields.io/badge/pmdarima-%234B8BBE.svg?style=for-the-badge" alt="pmdarima"></a>
  <a href="https://xgboost.readthedocs.io/" target="_blank"><img src="https://img.shields.io/badge/XGBoost-%230061A6.svg?style=for-the-badge&logo=xgboost&logoColor=white" alt="XGBoost"></a>
  <a href="https://lightgbm.readthedocs.io/" target="_blank"><img src="https://img.shields.io/badge/LightGBM-%238E44AD.svg?style=for-the-badge&logo=lightgbm&logoColor=white" alt="LightGBM"></a>
  <a href="https://matplotlib.org/" target="_blank"><img src="https://img.shields.io/badge/Matplotlib-%231f77b4.svg?style=for-the-badge&logo=matplotlib&logoColor=white" alt="Matplotlib"></a>
  <a href="https://seaborn.pydata.org/" target="_blank"><img src="https://img.shields.io/badge/Seaborn-%234C72B0.svg?style=for-the-badge&logo=seaborn&logoColor=white" alt="Seaborn"></a>
</p>

<p align="center">
  <a href="#14-license"> <img src="https://img.shields.io/badge/License-MIT-blue.svg?style=for-the-badge" alt="License: MIT">
  </a>
</p>

---

## üåü Project Vision & Strategic Imperative

In an era of unprecedented global market dynamism, achieving a **hyper-responsive, intelligently orchestrated, and data-centric supply chain** is paramount for sustained competitive dominance. **SupplyChain AI** (SCAI) represents a paradigm shift, designed to elevate traditional supply chain paradigms into sophisticated, predictive, and self-optimising operational ecosystems.

Our core mission is to compellingly demonstrate how the strategic application of **Artificial Intelligence (AI)**‚Äîinitially focused on **Advanced Demand Forecasting**, **Intelligent Inventory Optimisation**, and **Proactive Risk Intelligence**‚Äîempowers organisations to:

* Realise substantial **reductions in operational and inventory holding costs.**
* Achieve transformative **gains in end-to-end operational efficiency and throughput.**
* Cultivate robust **organisational resilience against market disruptions and demand volatility.**
* Secure a distinct and sustainable **competitive advantage in complex global markets.**

This project, developed entirely in Python and presented through an interactive Streamlit web application, demonstrates a cohesive, end-to-end data science workflow. This workflow encompasses data ingestion and rigorous preprocessing, sophisticated feature engineering, comprehensive model training and evaluation across diverse algorithmic families, and the deployment of insightful, user-friendly dashboards.

‚û°Ô∏è **Try the Live Demo:** [**supplychainai.streamlit.app**](https://supplychainai.streamlit.app/) ‚¨ÖÔ∏è

---

## üéØ Key Business Challenges Addressed

SupplyChain AI directly confronts critical pain points that hinder growth, constrict profitability, and diminish market responsiveness in modern supply chains:

* **Persistent Demand Uncertainty:** Overcoming the limitations of conventional forecasting in highly volatile environments, thereby minimising the financial and reputational impact of stockouts and excess inventory.
* **Suboptimal Capital Allocation:** Addressing excessive working capital tied up in inventory, reducing holding costs, and mitigating risks of product obsolescence or spoilage.
* **Fragmented Operational Visibility & Agility:** Countering the effects of siloed data and reactive decision-making to enhance the capacity for proactive adaptation to unforeseen global disruptions.
* **Escalating Competitive & Margin Pressures:** Providing crucial optimisation levers for businesses striving to enhance service delivery, reduce operational friction, and protect margins.

---

## üìà Quantifiable Business Impact & Value Proposition

By architecting AI-driven solutions to these challenges, SCAI offers a clear trajectory towards significant and measurable business outcomes:

* **Enhanced Financial Performance:** Driving profitability through strategic cost containment and revenue protection.
* **Elevated Customer Experience & Brand Loyalty:** Ensuring superior product availability and delivery reliability.
* **Optimised Operational Throughput:** Streamlining planning cycles and maximising resource utilisation.
* **Fortified Risk Management & Business Continuity:** Instilling foresight to anticipate vulnerabilities and agility to respond effectively.
* **Data-Driven Strategic Superiority:** Converting vast datasets into actionable, predictive intelligence for industry leadership.

---

## üõ†Ô∏è Core Modules & Key Functionalities

SCAI is architected around distinct, interconnected modules, each targeting a critical facet of modern supply chain management. The platform's insights are delivered through an interactive Streamlit application.


### 1. üìä Data Insights & Exploratory Analysis

* **Objective:** To provide a foundational understanding of the dataset through comprehensive Exploratory Data Analysis (EDA). This involves critically investigating data to discover patterns, spot anomalies, test hypotheses, and check assumptions.
* **Key Functionalities & Value:**
    * **Data Integrity & Overview:** Presents the structure of the cleaned Walmart sales dataset, column data types, non-null counts, and descriptive statistics for numerical and categorical features.
    * **Missing Value Assessment:** Details any residual missing values after initial cleaning. *Importance: Highlights potential data quality issues that could impact modelling.*
    * **Target Variable Analysis (`Weekly_Sales`):** In-depth analysis of the `Weekly_Sales` distribution, including skewness and kurtosis. *Importance: Crucial for understanding sales patterns, identifying outliers, and informing modelling choices (e.g., need for transformations).*
    * **Univariate Feature Analysis:** Interactive exploration of individual numerical (histograms, box plots) and categorical (count plots) features like Temperature, Fuel Price, Store Type, Holiday status. *Importance: Helps understand each feature's characteristics and potential influence on sales.*
    * **Bivariate Analysis:** Visualisation (scatter plots, box plots) of relationships between individual features and `Weekly_Sales`, including correlation coefficients. *Importance: Identifies features that are potentially strong predictors of sales.*
    * **Time Series Analysis:**
        * Overall aggregated sales trend and seasonal decomposition (Trend, Seasonality, Residuals). *Importance: Reveals long-term sales direction, regular yearly patterns, and random noise, critical for time series model selection.*
        * Granular sales trends for specific Store-Department combinations, including Autocorrelation (ACF) and Partial Autocorrelation (PACF) plots. *Importance: Uncovers micro-market dynamics and helps identify parameters for ARIMA-family models.*

### 2. üèÜ Model Performance & Comparative Insights

* **Objective:** To systematically compare the performance of various forecasting models trained on the sales data, enabling data-driven model selection.
* **Key Functionalities & Value:**
    * **Consolidated Experiment Logs:** Aggregates results from all trained models (classical time series, machine learning regression, deep learning).
    * **Key Performance Metrics Explained:** MAE, RMSE, MAPE are defined and their significance (lower is better) is highlighted.
    * **Summary Tables:**
        * *Average Metrics by Model Family:* Provides a high-level view of which general modelling approaches (e.g., SARIMA, Prophet, LSTM) perform best on average. *Importance: Guides initial model category selection.*
        * *Average Metrics by Specific Model Configuration:* Allows granular comparison of unique model setups. *Importance: Helps understand the impact of hyperparameter tuning.*
        * *Top Performing Model per Store-Department (by RMSE):* Identifies the best model for individual sales series. *Importance: Critical for deploying tailored, high-accuracy forecasting solutions rather than a one-size-fits-all approach.*
    * **Performance Visualisations:**
        * *Bar Charts (Average Metric by Model Family):* Quick visual comparison of average model family performance.
        * *Box Plots (Distribution of Metric by Model Family):* Reveals the spread, central tendency, and consistency of performance for each model family across various runs. *Importance: Assesses not just average performance but also reliability.*

### 3. üöÄ Forecast Visualiser & Model Explorer

* **Objective:** To enable in-depth qualitative analysis of individual forecasting models by visualising their predictions against actual historical data for specific Store-Department series.
* **Key Functionalities & Value:**
    * **Interactive Selection:** Users can choose a specific Store, Department, and a trained Model (from logged experiments, including global models) and define a future forecast horizon.
    * **Model Profile Display:** Shows logged test-set performance metrics (MAE, RMSE, MAPE) and key hyperparameters for the selected model. *Importance: Provides context on the model's original evaluated accuracy and configuration.*
    * **Forecast Visualisation (Currently with Placeholder Data for Predictions):**
        * Plots Actual Historical Sales.
        * Plots Actual Sales during the model's logged Test Period (if available from logs).
        * Displays (Placeholder) Model Predictions on the Test Period.
        * Displays (Placeholder) New Future Forecasts for the user-defined horizon.
        * *Importance: This visual comparison allows for qualitative assessment of how well the model captures trends, seasonality, and specific patterns, complementing quantitative metrics. The placeholder data needs to be replaced with actual model prediction logic for full functionality.*
    * **Troubleshooting for Model Loading:** Provides guidance if selected model artifacts cannot be loaded.

### 4. üßÆ Strategic Inventory Optimisation Engine 

* **Objective:** To translate demand characteristics (derived from forecasts or entered manually) into optimal inventory parameters and to simulate the performance of these inventory policies.
* **Key Functionalities & Value:**
    * **Forecast-Linked Demand Inputs:** Optionally uses a selected forecasting model's RMSE (as a proxy for demand standard deviation) and test-period actuals (for average demand) to inform calculations. *Importance: Leads to more data-driven and realistic inventory parameters.*
    * **Interactive Inventory Parameter Calculator:** Users input demand characteristics (average, std. dev.), costs (item, ordering, holding rate), lead time (average, std. dev.), and target service level. The calculator computes:
        * *Economic Order Quantity (EOQ):* The order size that minimises total ordering and holding costs.
        * *Safety Stock (SS):* Buffer stock to protect against demand and lead time variability for the desired service level.
        * *Reorder Point (ROP):* The inventory level at which a new order should be placed.
        * *Importance: Provides a statistical basis for setting inventory control levels.*
    * **Illustrative Annual Cost Breakdown:** Shows estimated annual ordering, cycle stock holding, and safety stock holding costs. *Importance: Helps understand the financial trade-offs of the inventory policy.*
    * **Inventory Policy Simulation:** Simulates inventory levels over time using the calculated (or custom) EOQ and ROP, against either constant average demand or actual historical demand for a selected Store-Department.
        * Displays key performance indicators: total demand, units sold, stockout units, fill rate, number of orders, average inventory.
        * Visualises inventory levels over time, showing ROP, SS, and stockout events.
        * *Importance: Allows for dynamic stress-testing of inventory policies and visual identification of potential issues before implementation.*
    * **Sensitivity Analysis:** Explores how EOQ, SS, and ROP change when a single key input parameter (e.g., holding cost, demand variability, lead time) is varied. Results shown in tables and plots. *Importance: Identifies critical parameters that most influence inventory policy, guiding where to focus data accuracy and risk mitigation efforts.*

### 5. üö® Proactive Supply Chain Risk Intelligence Engine 

* **Objective:** To demonstrate an AI-augmented approach for identifying, assessing, and categorising potential supply chain risks from unstructured textual data (using a sample news feed).
* **Key Functionalities & Value:**
    * **Value Proposition:** Explains how proactive risk detection minimizes disruptions, reduces costs, and enhances resilience.
    * **Sample News Data Processing:** Loads and displays sample news items.
    * **AI-Driven Enrichment:**
        * *VADER Sentiment Analysis:* Assigns Positive, Negative, or Neutral sentiment to news text.
        * *Risk Category Classification (with pre-trained model, if available):* Automatically categorizes news into types like "Logistics Disruption," "Natural Disaster."
        * *Dynamic AI Risk Score (0-10):* Combines predicted category severity and sentiment into a nuanced risk score.
        * *Importance: Transforms raw, unstructured information into structured, actionable risk intelligence.*
    * **Interactive Risk Alerts Dashboard:** Users can filter (by keyword, category, sentiment, AI score) and sort the processed news items. Each alert displays headline, summary, source, date, sentiment, AI category (vs. manual), and AI risk score with a visual delta. *Importance: Enables users to efficiently navigate and prioritise potential threats.*
    * **Future Enhancements:** Discusses integration of real-time feeds, advanced NLP, and richer visualisations for a production system.

---

## ‚öôÔ∏è High-Level Workflow

The platform follows a structured data science workflow:

1.  **Data Ingestion & Preparation:** Loading and initial cleaning of raw datasets (Walmart sales, stores, features; sample news).
2.  **Feature Engineering:** Crafting relevant features for demand forecasting (temporal, lags, rolling stats) and text preprocessing for NLP tasks.
3.  **Model Training & Evaluation:** Training diverse models (time series, ML regression, LSTMs for forecasting; text classifiers for risk) and systematically logging experiments and artifacts.
4.  **Inventory Calculation & Simulation:** Implementing core inventory models and simulating policy performance.
5.  **Interactive Application (Streamlit):** Presenting EDA, model comparisons, forecast exploration, inventory tools, and risk alerts through a user-friendly web interface.

---

## üíª Technology Stack

* **Programming Language:** Python (3.9+)
* **Core Data Science & ML:** Pandas, NumPy, Scikit-learn, Statsmodels, Pmdarima (for auto_arima), Prophet, XGBoost, LightGBM, TensorFlow, Keras (for LSTMs).
* **Natural Language Processing (NLP):** NLTK (VADER, tokenizers, stopwords).
* **Scientific Computing & Statistics:** SciPy (for statistical functions like Z-score).
* **Web Application Framework:** Streamlit, Streamlit Option Menu.
* **Data Visualisation:** Matplotlib, Seaborn, Plotly (used in specific Streamlit page refinements).
* **Utilities:** Joblib (for model persistence), `ast`, `re`, `datetime`, `os`, `sys`.

---

## üóÇÔ∏è Datasets Used

* **Walmart Sales Forecasting Data:**
    * **Source:** [Kaggle - Walmart Recruiting - Store Sales Forecasting competition](https://www.kaggle.com/c/walmart-recruiting-store-sales-forecasting).
    * **Files:** `train.csv`, `stores.csv`, `features.csv` (to be placed in `data/raw/`).
    * **Purpose:** Primary dataset for training and evaluating demand forecasting models and for deriving demand characteristics for inventory optimisation.
* **Sample Risk News Data:**
    * **File:** `sample_risk_news.csv` (in `data/raw/`).
    * **Content:** A curated set of sample news headlines/summaries with manual risk category labels.
    * **Purpose:** Used in the Risk Alerts module to demonstrate NLP-based sentiment analysis, AI category prediction, and dynamic risk scoring.

---

## üõ†Ô∏è Installation & Setup

### Prerequisites
* Python 3.9+
* `pip`
* Virtual environment manager (e.g., `venv`, Conda) - Highly Recommended.

### Steps

1.  **Clone the Repository (if applicable) or Download Files:**
    ```bash
    # If cloning:
    # git clone [YOUR_PROJECT_GITHUB_URL_HERE]
    # cd [PROJECT_DIRECTORY_NAME]
    ```
    Navigate to the project's root directory.

2.  **Create and Activate a Virtual Environment:**
    ```bash
    python3 -m venv .venv
    source .venv/bin/activate   # macOS/Linux
    # .venv\Scripts\activate    # Windows PowerShell
    # conda create -n supplychainenv python=3.9 -y && conda activate supplychainenv # Conda example
    ```

3.  **Install Dependencies:**
    ```bash
    pip install -r requirements.txt
    ```

4.  **Download NLTK Resources:**
    The application attempts to download these on first use if missing. To do it manually:
    ```python
    # Run in a Python interpreter within your activated environment:
    import nltk
    nltk.download('punkt')
    nltk.download('stopwords')
    nltk.download('vader_lexicon')
    ```

5.  **Prepare Data Files:**
    * Place Walmart `train.csv`, `features.csv`, `stores.csv` into the `data/raw/` directory.
    * Ensure `sample_risk_news.csv` is in `data/raw/`.
    * The main feature-engineered dataset (e.g., `walmart_data_featured.parquet`) is generated by the feature engineering script (see Usage).

---

## üöÄ Usage Instructions

1.  **Activate Virtual Environment.**
2.  **Navigate to Project Root Directory.**
3.  **Data Processing & Feature Engineering (Run once initially):**
    ```bash
    python src/features/demand_features.py
    ```
    This generates `walmart_data_featured.parquet` in `data/processed/`.

4.  **Train Models (Optional - if you want to regenerate models/logs):**
    Models and logs are expected to be pre-existing in `models_store/` and `reports/experiment_logs/`. To retrain:
    * **Demand Forecasting:**
        ```bash
        python src/models/demand_forecasting/classical_timeseries.py
        python src/models/demand_forecasting/classical_regression.py
        python src/models/demand_forecasting/deep_learning.py
        ```
    * **Risk Classification:**
        ```bash
        python src/models/risk_detection/train_risk_classifier.py
        ```
    *(Note: Training, especially LSTMs and `auto_arima`, can be time-consuming.)*

5.  **Run the Streamlit Application:**
    ```bash
    streamlit run app/main_app.py
    ```
    Or, to potentially avoid issues with some complex libraries during development:
    ```bash
    streamlit run app/main_app.py --server.fileWatcherType none
    ```
    Access the app via the local URL provided in the terminal (usually `http://localhost:8501`).

---

## üìà Business Applications & Value

This SCIP-X platform highlights several key business applications:

* **Improved Demand Planning:** More reliable forecasts reduce costly errors.
* **Optimised Inventory & Working Capital:** Data-driven inventory parameters improve cash flow and reduce holding costs/risks.
* **Enhanced Supply Chain Resilience:** Proactive risk identification allows for quicker mitigation and adaptation.
* **Data-Driven Decision Support:** Provides quantitative insights for informed strategic and operational choices.
* **Reduced Waste & Better Resource Allocation:** Minimises overstocking and improves efficiency.
* **Improved Cross-functional Collaboration:** A shared platform for insights can bridge departmental silos.

---

## ‚ö†Ô∏è Limitations

* **Risk Module Data:** Uses static, sample news data; a production system needs real-time, diverse feeds and larger training sets for NLP.
* **Exogenous Variable Forecasting:** Future forecasts assume simplistic future values for external regressors; these would need dedicated forecasting.
* **Model Generalizability:** Models are dataset-specific; retraining and continuous monitoring are vital for new contexts.
* **Real-time Capabilities:** Operates on static data; production use requires robust data pipelines for real-time/batch updates.
* **Simulated MLOps:** Incorporates logging and artifact saving, but lacks a full MLOps pipeline (automated retraining, CI/CD, advanced versioning).
* **Scalability:** Current stack may need distributed solutions (e.g., Spark) for extremely large datasets.

---

## üå± Future Work & Potential Enhancements

* **Advanced Risk Detection:** Real-time feeds, Transformer-based NLP, advanced risk scoring, geo-visualisations, alerting.
* **Supplier Performance Analytics Module:** Score supplier reliability, forecast supplier-specific risks.
* **Executive Dashboard & Scenario Planning:** Unified KPIs, "what-if" simulation tools.
* **Advanced Forecasting/Inventory Techniques:** Hierarchical forecasting, probabilistic forecasts, multi-echelon inventory optimisation.
* **Full MLOps Integration:** Automated pipelines (Kubeflow, MLflow), model monitoring.
* **User Management & Scalability:** Authentication, personalization, Dask/Spark exploration.

---

## üë®‚Äçüíª Developer

* **Name:** Ramesh Shrestha
* **Email:** shrestha.ramesh000@gmail.com
* **LinkedIn:** [linkedin.com/in/rameshsta](https://linkedin.com/in/rameshsta)
* **GitHub Project Repository:** `https://github.com/RameshSTA/SupplyChainAI` 
---

## üìÑ License

This project is licensed under the MIT License. See the `LICENSE` file for details.